# Dataset Views Generator

This folder contains the code to generate the dataset views from the original datasets.

## Install Dependencies

You need to install the dependencies listed in the `requirements.txt` file. You can do it by running the following command:

```bash
pip install -r requirements.txt
```

You also must have the datasets downloaded on the `preliminary_analisys/datasets_preprocessing/data/original` directory and run the python file `dataset_generator` that is used to generate the dataset views for all tasks.

## Datasets Directory Structure

From the root's git repository path, the datasets must be placed in the `preliminary_analisys/datasets_preprocessing/data/original` directory.
The datasets must have the following structure:

```
preliminary_analisys
└── datasets_preprocessing
    └── data
        ├── original
        |   ├── KuHar
        |   ├── MotionSense
        |   ├── RealWord
        |   ├── UCI
        |   ├── WISDM
        |   ├── RecodGait_v1
        |   ├── RecodGait
        |   ├── GaitOpticalInertial
        |   ├── UMAFall
        |   └── HAPT
        |
        ├── authentication
        |   ├── initial_dataset
        |   |   ├── RecodGait_v1
        |   |   ├── RecodGait
        |   |   └── GaitOpticalInertial  
        |   |
        |   ├── raw_balanced
        |   ├── standartized_balanced
        |   └── unbalanced
        |
        ├── fall
        |   ├── raw_balanced
        |   |   └── UMAFall
        |   |
        |   ├── standartized_balanced
        |   └── unbalanced
        |
        ├── har
        |   ├── raw_balanced
        |   │   ├── KuHar
        |   │   ├── MotionSense
        |   │   ├── RealWord
        |   |   ├── RealWorld_thigh
        |   |   ├── RealWorld_waist
        |   │   ├── UCI
        |   │   └── WISDM
        |   |
        |   ├── raw_balanced_user
        |   ├── standartized_balanced
        |   ├── standartized_balanced_user
        |   ├── standartized_cpc_balanced
        |   ├── standartized_inter_balanced
        |   ├── standartized_inter_balanced_user
        |   └── unbalanced
        └── transitions
            ├── initial_dataset_balanced
            |   ├── HAPT
            |   ├── HAPT_different_transitions
            |   └── HAPT_only_transitions
            |
            ├── initial_dataset_unbalanced
            ├── raw_balanced
            ├── raw_unbalanced
            ├── standartized_balanced
            ├── standartized_unbalanced
            └── unbalanced
```     

Where the `original` directory contains the original datasets (as they are downloaded), the folders `authentication`, `har`, `fall`, and `transitions` store the preprocessed data for each task methodology.

The folders `initial_dataset`, `raw_balanced`, `standartized_balanced`, `standartized_cpc_balanced`, and `unbalanced` will be generated by the `dataset_generator.py`.

The different views are the following:

- initial_view: dataset preprocessed as the view described by the authors to reproduce the results reported by some reliable paper and compare with them.

- raw_balanced: dataset with the minimum preprocess, and the data balanced

- standartized_balanced: the dataset preprocessed without gravity, accelerometer measure in m/s^2, sample rate equal to 20 Hz, and 3 seconds window.

- standartized_cpc_balanced: similar to the view standartized_balanced, but with no windowize.

- unbalanced: dataset with no train/validation/test splits.

---

The RealWorld, RealWorld_thigh, and RealWorld_waist directories have the following difference:

- RealWorld: The entire dataset is pre-processed.

- RealWorld_thigh: The dataset is filtered by samples with positions equal to the thigh before doing the pre-processing.

- RealWorld_waist: The dataset is filtered by samples with positions equal to the waist before doing the pre-processing.

---

The HAPT, HAPT_different_transitions, and HAPT_only_transitions directories have the following difference:

- HAPT: The entire dataset was pre-processed with all classes, but all the posture transitions were unified in a single class.

- HAPT_different_transitions: Similar to the dataset described before, but we separate the posture transitions into different classes per each one.

- HAPT_only_transitions: The dataset contains only posture transition samples, all the common HAR samples are removed.

---

**NOTE**

- To generate datasets, the original data must be placed in the `preliminary_analisys/datasets_preprocessing/data/original` directory.
- The `raw_balanced` and `standartized_balanced` directories will be generated by the `dataset_generator.py`.
- Do not add the datasets to the git repository. The datasets are too big, and it is not necessary to add them to the repository. The datasets can be downloaded from the sources or downloaded from [HIAAC M4 Datasets Drive](https://drive.google.com/drive/u/1/folders/1NF63hQu1hCpVU2GlxjLuE5EcxYm9i0EP). You can download the file original.zip file if you want to generate the views.
- The `preliminary_analisys/datasets_preprocessing/data` directory is ignored by git.

---
## Task Methodology Split Data

We generate different views to solve three different tasks, being them: `Human Activity Recognition`, `Authentication`, `Fall Detection`, and `Posture Transition`.
For each problem to solve we have a different methodology to preprocess and split the data.

- HAR: we split the data separating samples for different users in each set (train, validation, test). 
With that, we guarantee that there are no samples of the same user in two sets.
When the `view` is balanced we keep the same number of samples for each class in each set split.

- Authentication: for this task we create a fold by user and split the data in train, and test. The train, and test are separeted by sessions where session 1 and 2 indicates different days of data collect. 
We labeled the interested user samples by 1 (indicate that is the positive sample) and the other users by 0 (indicate an impostor).

- Fall Detection: for this task the problem is solved as anomaly detection, when we train One Class Classifiers, that is, the classifier is trained with only one class (human activity recognition samples) and on the test set is based in two classes: `har samples` and `fall samples`.

- Posture Transition: for this task, the problem is similar to HAR, but we have more classes that identify different posture transitions (e.g. sitting to standing, standing to lie down). We separate the data in train and test by `Leave One Subject Out` (LOSO) methodology, where we train with n-1 subjects and test in the other one.

---
## Dataset Generation Process

To generate the datasets, you can run the `dataset_generator.py` and `inter_dataset_balancer.py` files, which are in this directory. These files generates the `raw` and `standardized` views that can be unbalanced, intra_balanced (balanced per activity), and inter_balanced (balanced per activity and user), that is, all users have the same number of samples per activity. To generate inter_balanced views,  that balance each dataset based on the minimum number of classes per split (train, validation, or test), of all datasets, you must run the file `inter_dataset_balancer.py`.

The order is very important, to run `inter_dataset_balancer.py` you need the intra_balanced datasets, so if you don't have these views on your machine, you should first run `dataset_generator.py` file and then run the `inter_dataset_balancer.py` file.

## Standardization Process

The standardization process also called the standardization pipeline, comprises the execution of several steps (operations) per dataset.
Each dataset has its standardization pipeline, as they are different from each other. 
The operators are all defined in the `dataset_processor.py` file. The operators are defined as classes, and each class has a `__call__` method, which receives pandas dataframe and returns a pandas dataframe. The `__call__` method is the operator's implementation.

---

**NOTE**

- The order of the operators is important, as some operators may require columns that may be added from other operators (which must run before).
- Seldom, some operators may return multiple pandas Dataframes. 

---

The standardization codes from the `dataset_generator.py` file usually comprise the following steps (**this is not a strict rule**):

1. Load the datasets and generate a single pandas dataframe with all the data, where each row represents a single instant of capture (and now a window). The loading is a dataset-specific process. The dataframe generated **usually** have the following columns (**this is not a rule**):
- A column for the x-axis acceleration (`accel-x` column); y-axis acceleration (`accel-y` column); z-axis acceleration (`accel-z` column); gyroscope x-axis (`gyro-x` column); gyroscope y-axis (`gyro-y` column); gyroscope z-axis (`gyro-z` column); and the timestamp from the accelerometer (`accel-timestamp`) and gyroscope (`gyro-timestamp`), if provided.
- A column for the label (`activity code` column).
- A column for the user id (`user` column), if provided.
- A column for x-axis gravity (`gravity-x` column); y-axis gravity (`gravity-y` column); and z-axis gravity (`gravity-z` column) if provided.
- A serial column, which represents the attempt that the collection was made (`serial` column) if provided. For instance, if the user has a time series running in the morning and another in the afternoon, it will be two different serial numbers.
- A CSV or file column, which represents the file that the row was extracted from (`csv` column). 
- An index column, that is a column that represents the row index from the CSV file (`index` column).
- Any other column that may be useful for the standardization process or metadata.
2. Create the operator objects.
3. Crete the pipeline object, passing the operator object list as parameters.
4. Execute the pipeline, passing the dataframe as a parameter.

The following code snippet illustrates a fictitious standardization process for the `KuHar` dataset that resamples it to 20Hz and creates 3-second windows (**using this kind of code and operators is not a rule**):

```python

def read_kuhar(kuhar_dir_path: str) -> pd.DataFrame:
    # This is dataset-specific code. It reads the CSV files and generates a single dataframe with all the data.
    ... 
    return dataframe

# -----------------------------------------------------------------------------
# 1. Load the datasets and generate a single pandas dataframe with all the data
# -----------------------------------------------------------------------------

dataframe = read_kuhar("../data/original/KuHar/1.Raw_time_domian_data")

# -----------------------------------------------------------------------------
# 2. Create operaators
# -----------------------------------------------------------------------------

# List with columns that are features
feature_columns = [
    "accel-x",
    "accel-y",
    "accel-z",
    "gyro-x",
    "gyro-y",
    "gyro-z",
]


# Instantiate the object that resamples the data to 20Hz
# (suppose that the original dataset has a constant sample rate equal to 100Hz)
resampler = ResamplerPoly(
    features_to_select=feature_columns, # Name of the columns that will be used
                                        # as features.
    up=2,                               # The upsampling factor.
    down=10,                            # The downsampling factor.
    groupby_column="csv",               # Group by csv column. 
                                        # Resampling is done for each group of
                                        # csv column group.
)

# Instantiate the object that creates the windows
windowizer = Windowize(
    features_to_select=feature_columns, # Name of the columns that will be used
                                        # as features.
    samples_per_window=60,              # Number of samples per window.
    samples_per_overlap=0,              # Number of samples that overlap.
    groupby_column="csv",               # Group by csv column.
                                        # Resampling is done for each group of
                                        # csv column group.
)

# -----------------------------------------------------------------------------
# 3. Create the pipeline object, passing the operator object list as parameters
# -----------------------------------------------------------------------------

# Create the pipeline
# 1. Resample the data
# 2. Create the windows
# 3. Add a column with activity code
pipeline = Pipeline(
    [
        differ,
        resampler,
        windowizer,
        standard_label_adder
    ]
)

# -----------------------------------------------------------------------------
# 4. Execute the pipeline, passing the dataframe as a parameter
# -----------------------------------------------------------------------------

standartized_dataset = pipeline(dataframe)
```

The example above is to read and preprocess a dataset, in the `dataset_generator.py` file there is a dictionary with a specific pipeline for each dataset and view. 

The pipeline operators are usually shared with other datasets, as they are generic.